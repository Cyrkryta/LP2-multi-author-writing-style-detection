{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install nltk\n",
    "#!pip install textstat\n",
    "#!pip install cupy cuml\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36654\n",
      "5122\n",
      "5595\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Train: Medium + Hard\n",
    "df_train = pd.read_csv(\"FINAL-DATA/train_balanced.csv\")\n",
    "df_train = df_train.loc[df_train['difficulty'].isin(['medium', 'hard'])]\n",
    "print(len(df_train))\n",
    "\n",
    "# Validation: Medium + Hard\n",
    "df_val = pd.read_csv(\"FINAL-DATA/validation_balanced.csv\")\n",
    "df_val = df_val.loc[df_val['difficulty'].isin(['medium', 'hard'])]\n",
    "print(len(df_val))\n",
    "\n",
    "df_test = pd.read_csv(\"FINAL-DATA/test.csv\")\n",
    "print(len(df_test))\n",
    "\n",
    "df_test_easy = df_test[df_test[\"difficulty\"] == \"easy\"]\n",
    "df_test_medium = df_test[df_test[\"difficulty\"] == \"medium\"]\n",
    "df_test_hard = df_test[df_test[\"difficulty\"] == \"hard\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.reset_index(inplace= True)\n",
    "df_test_medium.reset_index(inplace= True)\n",
    "df_test_hard.reset_index(inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for extracting features and calculating similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\milos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\milos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\milos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\milos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import cosine\n",
    "from nltk.metrics.distance import jaccard_distance\n",
    "import numpy as np\n",
    "import textstat\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\n",
    "#Preprocessing, required for some features\n",
    "def preprocess_text(text):\n",
    "    # Tokenize text into words\n",
    "    words = word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    return words\n",
    "\n",
    "\n",
    "#Feature 1 = word frequency, dict\n",
    "def extract_word_freq_features(paragraph):\n",
    "    # Preprocess the paragraph\n",
    "    words = preprocess_text(paragraph)\n",
    "    \n",
    "    # Count word frequencies\n",
    "    word_freq = Counter(words)\n",
    "    \n",
    "    return word_freq\n",
    "\n",
    "\n",
    "#Feature 2 = tfidf, float\n",
    "def extract_tfidf_features(paragraph1, paragraph2):\n",
    "    # Create TF-IDF vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer(tokenizer=preprocess_text)\n",
    "    \n",
    "    # Fit and transform paragraphs\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform([paragraph1, paragraph2])\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarity_score = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])[0][0]\n",
    "    \n",
    "    return similarity_score\n",
    "\n",
    "\n",
    "\n",
    "# Feature 3 = avg_sent_len, int\n",
    "# Feature 4 = punctuation counts, dict\n",
    "def extract_sentence_structure_features(paragraph):\n",
    "    # Tokenize the paragraph into sentences\n",
    "    sentences = sent_tokenize(paragraph)\n",
    "    avg_sentence_length = sum(len(word_tokenize(sent)) for sent in sentences) / len(sentences)\n",
    "    punctuation_counts = Counter(token for sent in sentences for token in word_tokenize(sent) if token in (',', '.', '!', '?'))\n",
    "    \n",
    "    return avg_sentence_length, punctuation_counts\n",
    "\n",
    "# Feature 5 = pos_tags\n",
    "def extract_pos_tag_features(paragraph):\n",
    "    # Tokenize the paragraph into words\n",
    "    words = word_tokenize(paragraph)\n",
    "    \n",
    "    # Get POS tags\n",
    "    pos_tags = pos_tag(words)\n",
    "    \n",
    "    # Count POS tag frequencies\n",
    "    pos_tag_freq = Counter(tag for word, tag in pos_tags)\n",
    "    \n",
    "    return pos_tag_freq\n",
    "\n",
    "# Feature 6 = Reading Ease\n",
    "def extract_reading_ease(paragraph):\n",
    "    flesch_reading_ease = textstat.flesch_reading_ease(paragraph)\n",
    "    return flesch_reading_ease\n",
    "\n",
    "# Feature 7 = whole tfidf\n",
    "def create_tfidf_matrix(series):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(series)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    df_sparse = pd.DataFrame.sparse.from_spmatrix(tfidf_matrix, columns=feature_names)\n",
    "    return df_sparse\n",
    "\n",
    "def tfidf_similarities(df1, df2):\n",
    "    '''\n",
    "    cos_sims = []\n",
    "    for i in range(100):\n",
    "        cos_sim = cosine_similarity(np.vstack([tfidf_matrix_1_filtered.iloc[i], tfidf_matrix_2_filtered.iloc[i]]), dense_output=False)\n",
    "        cos_sims.append(cos_sim[0][1])\n",
    "    '''\n",
    "    norms1 = np.linalg.norm(df1, axis=1)\n",
    "    norms2 = np.linalg.norm(df2, axis=1)\n",
    "\n",
    "    # Ensure no division by zero\n",
    "    epsilon = 1e-10\n",
    "    norms1 = np.maximum(norms1, epsilon)\n",
    "    norms2 = np.maximum(norms2, epsilon)\n",
    "\n",
    "    # Calculate the dot product of corresponding rows\n",
    "    dot_products = np.einsum('ij,ij->i', df1, df2)\n",
    "\n",
    "    # Calculate the cosine similarity\n",
    "    cosine_similarity_vector = dot_products / (norms1 * norms2)\n",
    "    return cosine_similarity_vector\n",
    "\n",
    "# Functions for computing similarity between paragraphs\n",
    "def compare_numerical_features(feature1, feature2):\n",
    "    # Compute cosine similarity between feature vectors\n",
    "    return 1 - cosine(feature1, feature2)\n",
    "\n",
    "def compare_categorical_features(feature1, feature2):\n",
    "    # Compute Jaccard similarity between sets\n",
    "    if len(set(feature1.keys())) == 0 and len(set(feature2.keys())) == 0:\n",
    "        return 0\n",
    "    return 1 - jaccard_distance(set(feature1.keys()), set(feature2.keys()))  \n",
    "    \n",
    "\n",
    "def extract_similarity(para_1, para_2):\n",
    "    # Word freq (jac_sim)\n",
    "    word_freq_features1 = extract_word_freq_features(para_1)\n",
    "    word_freq_features2 = extract_word_freq_features(para_2)\n",
    "    \n",
    "    jaccard_similarity_word_freqs = compare_categorical_features(word_freq_features1, word_freq_features2)\n",
    "\n",
    "    # tfidf (cos_sim)\n",
    "    tf_idf_similarity_score = extract_tfidf_features(para_1, para_2)\n",
    "\n",
    "    # Sent structure:\n",
    "    avg_sentence_length_1, punctuation_counts_1 = extract_sentence_structure_features(para_1)\n",
    "    avg_sentence_length_2, punctuation_counts_2 = extract_sentence_structure_features(para_2)\n",
    "\n",
    "    #sent len (proportion):\n",
    "    sent_len_diff = (avg_sentence_length_2/avg_sentence_length_1 if avg_sentence_length_1 != 0 else 0)\n",
    "\n",
    "    #punctuation difference (jac_sim):\n",
    "    jaccard_similarity_punct = compare_categorical_features(punctuation_counts_1, punctuation_counts_2)\n",
    "\n",
    "    #pos_tag (jac_sim):\n",
    "    pos_tag_features_1 = extract_pos_tag_features(para_1)\n",
    "    pos_tag_features_2 = extract_pos_tag_features(para_2)\n",
    "    jaccard_similarity_punct = compare_categorical_features(pos_tag_features_1, pos_tag_features_2)\n",
    "\n",
    "\n",
    "    return jaccard_similarity_word_freqs, tf_idf_similarity_score, sent_len_diff, jaccard_similarity_punct, jaccard_similarity_punct\n",
    "\n",
    "def extract_ease_sim(para1, para2):\n",
    "    ease_1 = extract_reading_ease(para1)\n",
    "    ease_2 = extract_reading_ease(para2)\n",
    "\n",
    "    return ease_2 - ease_1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_base_features(df):\n",
    "    #df_100 = df.head(100)\n",
    "    df['f1'] = pd.Series()\n",
    "    df['f2'] = pd.Series()\n",
    "    df['f3'] = pd.Series()\n",
    "    df['f4'] = pd.Series()\n",
    "    df['f5'] = pd.Series()\n",
    "    df['f6'] = pd.Series()\n",
    "\n",
    "    f1 = []\n",
    "    f2 = []\n",
    "    f3 = []\n",
    "    f4 = []\n",
    "    f5 = []\n",
    "    f6 = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        features_in_row = list(extract_similarity(df[\"paragraph1\"][i], df[\"paragraph2\"][i]))\n",
    "        f1.append(features_in_row[0])\n",
    "        f2.append(features_in_row[1])\n",
    "        f3.append(features_in_row[2])\n",
    "        f4.append(features_in_row[3])\n",
    "        f5.append(features_in_row[4]) \n",
    "        f6_val  = extract_ease_sim(df[\"paragraph1\"][i], df[\"paragraph2\"][i])\n",
    "        f6.append(f6_val)\n",
    "\n",
    "    df['f1'] = f1\n",
    "    df['f2'] = f2\n",
    "    df['f3'] = f3\n",
    "    df['f4'] = f4\n",
    "    df['f5'] = f5\n",
    "    df['f6'] = f6\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tfidf on whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tfidf_col(df):\n",
    "    tfidf_matrix_1 = create_tfidf_matrix(df[\"paragraph1\"])\n",
    "    tfidf_matrix_2 = create_tfidf_matrix(df[\"paragraph2\"])\n",
    "\n",
    "    common_columns = tfidf_matrix_1.columns.intersection(tfidf_matrix_2.columns)\n",
    "\n",
    "\n",
    "    #tfidf_matrix_1 = tfidf_matrix_1.head(10000)\n",
    "    #tfidf_matrix_2 = tfidf_matrix_2.head(10000)\n",
    "\n",
    "    tfidf_matrix_1_filtered = tfidf_matrix_1[common_columns].astype(np.float16)\n",
    "    tfidf_matrix_2_filtered = tfidf_matrix_2[common_columns].astype(np.float16)\n",
    "\n",
    "\n",
    "    cosine_similarities = tfidf_similarities(tfidf_matrix_1_filtered, tfidf_matrix_2_filtered)\n",
    "\n",
    "    df['f7'] = cosine_similarities\n",
    "\n",
    "    df['f7'] = df['f7'].fillna(0) \n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_all_features(df):\n",
    "    calculate_base_features(df)\n",
    "    calculate_tfidf_col(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\milos\\AppData\\Local\\Temp\\ipykernel_61380\\507688281.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['f1'] = pd.Series()\n",
      "C:\\Users\\milos\\AppData\\Local\\Temp\\ipykernel_61380\\507688281.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['f2'] = pd.Series()\n",
      "C:\\Users\\milos\\AppData\\Local\\Temp\\ipykernel_61380\\507688281.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['f3'] = pd.Series()\n",
      "C:\\Users\\milos\\AppData\\Local\\Temp\\ipykernel_61380\\507688281.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['f4'] = pd.Series()\n",
      "C:\\Users\\milos\\AppData\\Local\\Temp\\ipykernel_61380\\507688281.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['f5'] = pd.Series()\n",
      "C:\\Users\\milos\\AppData\\Local\\Temp\\ipykernel_61380\\507688281.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['f6'] = pd.Series()\n",
      "c:\\Users\\milos\\miniconda3\\envs\\prpro-2024\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test_features_dataframe \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_all_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_test_medium\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m test_features_dataframe\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_features_dataframe_medium.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[27], line 2\u001b[0m, in \u001b[0;36mcalculate_all_features\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_all_features\u001b[39m(df):\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mcalculate_base_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     calculate_tfidf_col(df)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "Cell \u001b[1;32mIn[25], line 18\u001b[0m, in \u001b[0;36mcalculate_base_features\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     15\u001b[0m f6 \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(df)):\n\u001b[1;32m---> 18\u001b[0m     features_in_row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43mextract_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparagraph1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparagraph2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     19\u001b[0m     f1\u001b[38;5;241m.\u001b[39mappend(features_in_row[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     20\u001b[0m     f2\u001b[38;5;241m.\u001b[39mappend(features_in_row[\u001b[38;5;241m1\u001b[39m])\n",
      "Cell \u001b[1;32mIn[24], line 136\u001b[0m, in \u001b[0;36mextract_similarity\u001b[1;34m(para_1, para_2)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_similarity\u001b[39m(para_1, para_2):\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;66;03m# Word freq (jac_sim)\u001b[39;00m\n\u001b[1;32m--> 136\u001b[0m     word_freq_features1 \u001b[38;5;241m=\u001b[39m \u001b[43mextract_word_freq_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpara_1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m     word_freq_features2 \u001b[38;5;241m=\u001b[39m extract_word_freq_features(para_2)\n\u001b[0;32m    139\u001b[0m     jaccard_similarity_word_freqs \u001b[38;5;241m=\u001b[39m compare_categorical_features(word_freq_features1, word_freq_features2)\n",
      "Cell \u001b[1;32mIn[24], line 41\u001b[0m, in \u001b[0;36mextract_word_freq_features\u001b[1;34m(paragraph)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_word_freq_features\u001b[39m(paragraph):\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# Preprocess the paragraph\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m     words \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparagraph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;66;03m# Count word frequencies\u001b[39;00m\n\u001b[0;32m     44\u001b[0m     word_freq \u001b[38;5;241m=\u001b[39m Counter(words)\n",
      "Cell \u001b[1;32mIn[24], line 28\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     25\u001b[0m words \u001b[38;5;241m=\u001b[39m word_tokenize(text\u001b[38;5;241m.\u001b[39mlower())\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Remove stopwords\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[43mstopwords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwords\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43menglish\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     29\u001b[0m words \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Lemmatization\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\milos\\miniconda3\\envs\\prpro-2024\\Lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py:21\u001b[0m, in \u001b[0;36mWordListCorpusReader.words\u001b[1;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwords\u001b[39m(\u001b[38;5;28mself\u001b[39m, fileids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, ignore_lines_startswith\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m     20\u001b[0m         line\n\u001b[1;32m---> 21\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m line_tokenize(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfileids\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line\u001b[38;5;241m.\u001b[39mstartswith(ignore_lines_startswith)\n\u001b[0;32m     23\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\milos\\miniconda3\\envs\\prpro-2024\\Lib\\site-packages\\nltk\\corpus\\reader\\api.py:218\u001b[0m, in \u001b[0;36mCorpusReader.raw\u001b[1;34m(self, fileids)\u001b[0m\n\u001b[0;32m    216\u001b[0m contents \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fileids:\n\u001b[1;32m--> 218\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[0;32m    219\u001b[0m         contents\u001b[38;5;241m.\u001b[39mappend(fp\u001b[38;5;241m.\u001b[39mread())\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m concat(contents)\n",
      "File \u001b[1;32mc:\\Users\\milos\\miniconda3\\envs\\prpro-2024\\Lib\\site-packages\\nltk\\corpus\\reader\\api.py:231\u001b[0m, in \u001b[0;36mCorpusReader.open\u001b[1;34m(self, file)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;124;03mReturn an open stream that can be used to read the given file.\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;124;03mIf the file's encoding is not None, then the stream will\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;124;03m:param file: The file identifier of the file to read.\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    230\u001b[0m encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding(file)\n\u001b[1;32m--> 231\u001b[0m stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_root\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen(encoding)\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stream\n",
      "File \u001b[1;32mc:\\Users\\milos\\miniconda3\\envs\\prpro-2024\\Lib\\site-packages\\nltk\\data.py:334\u001b[0m, in \u001b[0;36mFileSystemPathPointer.join\u001b[1;34m(self, fileid)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjoin\u001b[39m(\u001b[38;5;28mself\u001b[39m, fileid):\n\u001b[0;32m    333\u001b[0m     _path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path, fileid)\n\u001b[1;32m--> 334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFileSystemPathPointer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\milos\\miniconda3\\envs\\prpro-2024\\Lib\\site-packages\\nltk\\compat.py:41\u001b[0m, in \u001b[0;36mpy3_data.<locals>._decorator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decorator\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     40\u001b[0m     args \u001b[38;5;241m=\u001b[39m (args[\u001b[38;5;241m0\u001b[39m], add_py3_data(args[\u001b[38;5;241m1\u001b[39m])) \u001b[38;5;241m+\u001b[39m args[\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minit_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\milos\\miniconda3\\envs\\prpro-2024\\Lib\\site-packages\\nltk\\data.py:311\u001b[0m, in \u001b[0;36mFileSystemPathPointer.__init__\u001b[1;34m(self, _path)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;124;03mCreate a new path pointer for the given absolute path.\u001b[39;00m\n\u001b[0;32m    306\u001b[0m \n\u001b[0;32m    307\u001b[0m \u001b[38;5;124;03m:raise IOError: If the given path does not exist.\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    310\u001b[0m _path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(_path)\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexists\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_path\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo such file or directory: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m _path)\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path \u001b[38;5;241m=\u001b[39m _path\n",
      "File \u001b[1;32m<frozen genericpath>:19\u001b[0m, in \u001b[0;36mexists\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_features_dataframe = calculate_all_features(df_test_medium)\n",
    "test_features_dataframe.to_csv(\"test_features_dataframe_medium.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\milos\\AppData\\Local\\Temp\\ipykernel_61380\\507688281.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['f1'] = pd.Series()\n",
      "C:\\Users\\milos\\AppData\\Local\\Temp\\ipykernel_61380\\507688281.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['f2'] = pd.Series()\n",
      "C:\\Users\\milos\\AppData\\Local\\Temp\\ipykernel_61380\\507688281.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['f3'] = pd.Series()\n",
      "C:\\Users\\milos\\AppData\\Local\\Temp\\ipykernel_61380\\507688281.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['f4'] = pd.Series()\n",
      "C:\\Users\\milos\\AppData\\Local\\Temp\\ipykernel_61380\\507688281.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['f5'] = pd.Series()\n",
      "C:\\Users\\milos\\AppData\\Local\\Temp\\ipykernel_61380\\507688281.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['f6'] = pd.Series()\n",
      "c:\\Users\\milos\\miniconda3\\envs\\prpro-2024\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\milos\\AppData\\Local\\Temp\\ipykernel_61380\\507688281.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['f1'] = f1\n",
      "C:\\Users\\milos\\AppData\\Local\\Temp\\ipykernel_61380\\507688281.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['f2'] = f2\n",
      "C:\\Users\\milos\\AppData\\Local\\Temp\\ipykernel_61380\\507688281.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['f3'] = f3\n",
      "C:\\Users\\milos\\AppData\\Local\\Temp\\ipykernel_61380\\507688281.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['f4'] = f4\n",
      "C:\\Users\\milos\\AppData\\Local\\Temp\\ipykernel_61380\\507688281.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['f5'] = f5\n",
      "C:\\Users\\milos\\AppData\\Local\\Temp\\ipykernel_61380\\507688281.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['f6'] = f6\n",
      "C:\\Users\\milos\\AppData\\Local\\Temp\\ipykernel_61380\\265999880.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['f7'] = cosine_similarities\n",
      "C:\\Users\\milos\\AppData\\Local\\Temp\\ipykernel_61380\\265999880.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['f7'] = df['f7'].fillna(0)\n"
     ]
    }
   ],
   "source": [
    "test_features_dataframe_hard = calculate_all_features(df_test_hard)\n",
    "test_features_dataframe_hard.to_csv(\"test_features_dataframe_hard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\milos\\miniconda3\\envs\\prpro-2024\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\milos\\AppData\\Local\\Temp\\ipykernel_61380\\432453072.py:119: RuntimeWarning: invalid value encountered in divide\n",
      "  cosine_similarity_vector = dot_products / (norms1 * norms2)\n"
     ]
    }
   ],
   "source": [
    "train_features_dataframe = calculate_all_features(df_train)\n",
    "train_features_dataframe.to_csv(\"train_features_dataframe.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    18327\n",
      "1    18327\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_features_dataframe['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    18327\n",
      "1    18327\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "'''\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Check the new class distribution\n",
    "print(pd.Series(y_resampled).value_counts())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def prep_data_for_model(train_df, test_df):\n",
    "    X_train = train_df[[\"f1\", \"f2\", \"f3\", \"f4\", \"f5\", \"f6\", \"f7\"]]\n",
    "    y_train = train_df[\"label\"]\n",
    "\n",
    "    X_test = test_df[[\"f1\", \"f2\", \"f3\", \"f4\", \"f5\", \"f6\", \"f7\"]]\n",
    "    y_test = test_df[\"label\"]\n",
    "\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Fit the scaler on the training data and transform it\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "    # Transform the test data\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    #df.head(10)[[\"f1\", \"f2\", \"f3\", \"f4\", \"f5\"]]\n",
    "    return [X_train_scaled, X_test_scaled, y_train, y_test]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First model - linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def logistic_regression(data_list):\n",
    "    X_train, X_test, y_train, y_test = data_list[0], data_list[1], data_list[2], data_list[3]\n",
    "    model = LogisticRegression()\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    f1_score_reg = f1_score(y_test, predictions)\n",
    "    return f1_score_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second model - SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "def svm(data_list, kernels = ['rbf']):\n",
    "    X_train, X_test, y_train, y_test = data_list[0], data_list[1], data_list[2], data_list[3]\n",
    "    kernel_scores = {}\n",
    "    for kernel in kernels:\n",
    "        svm_model = SVC(kernel= kernel)  \n",
    "        svm_model.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = svm_model.predict(X_test)\n",
    "\n",
    "        f1_score_svm = f1_score(y_test, y_pred)\n",
    "        kernel_scores[kernel] = f1_score_svm\n",
    "\n",
    "    return kernel_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression f1 score, medium: 0.6965648854961832\n",
      "logistic regression f1 score, hard: 0.545751633986928\n",
      "svm f1 score, medium: {'rbf': 0.773851590106007}\n",
      "svm f1 score, hard: {'rbf': 0.5743695316520844}\n"
     ]
    }
   ],
   "source": [
    "print(f\"logistic regression f1 score, medium: {logistic_regression(prep_data_for_model(train_features_dataframe, test_features_dataframe))}\")\n",
    "print(f\"logistic regression f1 score, hard: {logistic_regression(prep_data_for_model(train_features_dataframe, test_features_dataframe_hard))}\")\n",
    "\n",
    "print(f\"svm f1 score, medium: {svm(prep_data_for_model(train_features_dataframe, test_features_dataframe))}\")\n",
    "print(f\"svm f1 score, hard: {svm(prep_data_for_model(train_features_dataframe, test_features_dataframe_hard))}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prpro-2024",
   "language": "python",
   "name": "prpro-2024"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
