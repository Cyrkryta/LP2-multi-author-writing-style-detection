{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install nltk\n",
    "#!pip install textstat\n",
    "#!pip install cupy cuml\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36654\n",
      "5122\n",
      "5595\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Train: Medium + Hard\n",
    "df_train = pd.read_csv(\"FINAL-DATA/train_balanced.csv\")\n",
    "df_train = df_train.loc[df_train['difficulty'].isin(['medium', 'hard'])]\n",
    "print(len(df_train))\n",
    "\n",
    "# Validation: Medium + Hard\n",
    "df_val = pd.read_csv(\"FINAL-DATA/validation_balanced.csv\")\n",
    "df_val = df_val.loc[df_val['difficulty'].isin(['medium', 'hard'])]\n",
    "print(len(df_val))\n",
    "\n",
    "df_test = pd.read_csv(\"FINAL-DATA/test.csv\")\n",
    "print(len(df_test))\n",
    "\n",
    "df_test_easy = df_test[df_test[\"difficulty\"] == \"easy\"]\n",
    "df_test_medium = df_test[df_test[\"difficulty\"] == \"medium\"]\n",
    "df_test_hard = df_test[df_test[\"difficulty\"] == \"hard\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph1</th>\n",
       "      <th>paragraph2</th>\n",
       "      <th>label</th>\n",
       "      <th>difficulty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1865</th>\n",
       "      <td>What an incredible silly take. Illhan had legi...</td>\n",
       "      <td>I think the point was missed. I’m not taking a...</td>\n",
       "      <td>0</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1866</th>\n",
       "      <td>I am a bot, and this action was performed auto...</td>\n",
       "      <td>A jib is the smaller front sail on a sailboat....</td>\n",
       "      <td>1</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867</th>\n",
       "      <td>Oh yeah. Totally have access to the LAPD entir...</td>\n",
       "      <td>He does a drug raid and it’s like, imagine the...</td>\n",
       "      <td>1</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1868</th>\n",
       "      <td>Oh okay that's quite interesting. This was dur...</td>\n",
       "      <td>There was that Comey guy and I think someone e...</td>\n",
       "      <td>0</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1869</th>\n",
       "      <td>You are supposed to compromise with people of ...</td>\n",
       "      <td>The GOP do not talk about Democrats like they ...</td>\n",
       "      <td>0</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3725</th>\n",
       "      <td>“Did you ever wanted to run a marathon, but di...</td>\n",
       "      <td>Coming soon from Philip Morris, Marlboro 5k's....</td>\n",
       "      <td>1</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3726</th>\n",
       "      <td>Same, also same way most Americans want to sav...</td>\n",
       "      <td>And voting doesn't really help. You have a two...</td>\n",
       "      <td>1</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3727</th>\n",
       "      <td>This makes sense. The only reason Trump became...</td>\n",
       "      <td>their thing is denialism, not trumpism. refusi...</td>\n",
       "      <td>1</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3728</th>\n",
       "      <td>You say that now, but you're operating with 20...</td>\n",
       "      <td>I can't imagine why, in a year that was expect...</td>\n",
       "      <td>0</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3729</th>\n",
       "      <td>Despite no longer being general secretary, Jia...</td>\n",
       "      <td>If you criticize Xi for underhandedly cementin...</td>\n",
       "      <td>0</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1865 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             paragraph1  \\\n",
       "1865  What an incredible silly take. Illhan had legi...   \n",
       "1866  I am a bot, and this action was performed auto...   \n",
       "1867  Oh yeah. Totally have access to the LAPD entir...   \n",
       "1868  Oh okay that's quite interesting. This was dur...   \n",
       "1869  You are supposed to compromise with people of ...   \n",
       "...                                                 ...   \n",
       "3725  “Did you ever wanted to run a marathon, but di...   \n",
       "3726  Same, also same way most Americans want to sav...   \n",
       "3727  This makes sense. The only reason Trump became...   \n",
       "3728  You say that now, but you're operating with 20...   \n",
       "3729  Despite no longer being general secretary, Jia...   \n",
       "\n",
       "                                             paragraph2  label difficulty  \n",
       "1865  I think the point was missed. I’m not taking a...      0     medium  \n",
       "1866  A jib is the smaller front sail on a sailboat....      1     medium  \n",
       "1867  He does a drug raid and it’s like, imagine the...      1     medium  \n",
       "1868  There was that Comey guy and I think someone e...      0     medium  \n",
       "1869  The GOP do not talk about Democrats like they ...      0     medium  \n",
       "...                                                 ...    ...        ...  \n",
       "3725  Coming soon from Philip Morris, Marlboro 5k's....      1     medium  \n",
       "3726  And voting doesn't really help. You have a two...      1     medium  \n",
       "3727  their thing is denialism, not trumpism. refusi...      1     medium  \n",
       "3728  I can't imagine why, in a year that was expect...      0     medium  \n",
       "3729  If you criticize Xi for underhandedly cementin...      0     medium  \n",
       "\n",
       "[1865 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_medium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for extracting features and calculating similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\milos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\milos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\milos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\milos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import cosine\n",
    "from nltk.metrics.distance import jaccard_distance\n",
    "import numpy as np\n",
    "import textstat\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\n",
    "#Preprocessing, required for some features\n",
    "def preprocess_text(text):\n",
    "    # Tokenize text into words\n",
    "    words = word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    return words\n",
    "\n",
    "\n",
    "#Feature 1 = word frequency, dict\n",
    "def extract_word_freq_features(paragraph):\n",
    "    # Preprocess the paragraph\n",
    "    words = preprocess_text(paragraph)\n",
    "    \n",
    "    # Count word frequencies\n",
    "    word_freq = Counter(words)\n",
    "    \n",
    "    return word_freq\n",
    "\n",
    "\n",
    "#Feature 2 = tfidf, float\n",
    "def extract_tfidf_features(paragraph1, paragraph2):\n",
    "    # Create TF-IDF vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer(tokenizer=preprocess_text)\n",
    "    \n",
    "    # Fit and transform paragraphs\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform([paragraph1, paragraph2])\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarity_score = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])[0][0]\n",
    "    \n",
    "    return similarity_score\n",
    "\n",
    "\n",
    "\n",
    "# Feature 3 = avg_sent_len, int\n",
    "# Feature 4 = punctuation counts, dict\n",
    "def extract_sentence_structure_features(paragraph):\n",
    "    # Tokenize the paragraph into sentences\n",
    "    sentences = sent_tokenize(paragraph)\n",
    "    avg_sentence_length = sum(len(word_tokenize(sent)) for sent in sentences) / len(sentences)\n",
    "    punctuation_counts = Counter(token for sent in sentences for token in word_tokenize(sent) if token in (',', '.', '!', '?'))\n",
    "    \n",
    "    return avg_sentence_length, punctuation_counts\n",
    "\n",
    "# Feature 5 = pos_tags\n",
    "def extract_pos_tag_features(paragraph):\n",
    "    # Tokenize the paragraph into words\n",
    "    words = word_tokenize(paragraph)\n",
    "    \n",
    "    # Get POS tags\n",
    "    pos_tags = pos_tag(words)\n",
    "    \n",
    "    # Count POS tag frequencies\n",
    "    pos_tag_freq = Counter(tag for word, tag in pos_tags)\n",
    "    \n",
    "    return pos_tag_freq\n",
    "\n",
    "# Feature 6 = Reading Ease\n",
    "def extract_reading_ease(paragraph):\n",
    "    flesch_reading_ease = textstat.flesch_reading_ease(paragraph)\n",
    "    return flesch_reading_ease\n",
    "\n",
    "# Feature 7 = whole tfidf\n",
    "def create_tfidf_matrix(series):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(series)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    df_sparse = pd.DataFrame.sparse.from_spmatrix(tfidf_matrix, columns=feature_names)\n",
    "    return df_sparse\n",
    "\n",
    "def tfidf_similarities(df1, df2):\n",
    "    '''\n",
    "    cos_sims = []\n",
    "    for i in range(100):\n",
    "        cos_sim = cosine_similarity(np.vstack([tfidf_matrix_1_filtered.iloc[i], tfidf_matrix_2_filtered.iloc[i]]), dense_output=False)\n",
    "        cos_sims.append(cos_sim[0][1])\n",
    "    '''\n",
    "    norms1 = np.linalg.norm(df1, axis=1)\n",
    "    norms2 = np.linalg.norm(df2, axis=1)\n",
    "\n",
    "    # Ensure no division by zero\n",
    "    epsilon = 1e-10\n",
    "    norms1 = np.maximum(norms1, epsilon)\n",
    "    norms2 = np.maximum(norms2, epsilon)\n",
    "\n",
    "    # Calculate the dot product of corresponding rows\n",
    "    dot_products = np.einsum('ij,ij->i', df1, df2)\n",
    "\n",
    "    # Calculate the cosine similarity\n",
    "    cosine_similarity_vector = dot_products / (norms1 * norms2)\n",
    "    return cosine_similarity_vector\n",
    "\n",
    "# Functions for computing similarity between paragraphs\n",
    "def compare_numerical_features(feature1, feature2):\n",
    "    # Compute cosine similarity between feature vectors\n",
    "    return 1 - cosine(feature1, feature2)\n",
    "\n",
    "def compare_categorical_features(feature1, feature2):\n",
    "    # Compute Jaccard similarity between sets\n",
    "    if len(set(feature1.keys())) == 0 and len(set(feature2.keys())) == 0:\n",
    "        return 0\n",
    "    return 1 - jaccard_distance(set(feature1.keys()), set(feature2.keys()))  \n",
    "    \n",
    "\n",
    "def extract_similarity(para_1, para_2):\n",
    "    # Word freq (jac_sim)\n",
    "    word_freq_features1 = extract_word_freq_features(para_1)\n",
    "    word_freq_features2 = extract_word_freq_features(para_2)\n",
    "    \n",
    "    jaccard_similarity_word_freqs = compare_categorical_features(word_freq_features1, word_freq_features2)\n",
    "\n",
    "    # tfidf (cos_sim)\n",
    "    tf_idf_similarity_score = extract_tfidf_features(para_1, para_2)\n",
    "\n",
    "    # Sent structure:\n",
    "    avg_sentence_length_1, punctuation_counts_1 = extract_sentence_structure_features(para_1)\n",
    "    avg_sentence_length_2, punctuation_counts_2 = extract_sentence_structure_features(para_2)\n",
    "\n",
    "    #sent len (proportion):\n",
    "    sent_len_diff = (avg_sentence_length_2/avg_sentence_length_1 if avg_sentence_length_1 != 0 else 0)\n",
    "\n",
    "    #punctuation difference (jac_sim):\n",
    "    jaccard_similarity_punct = compare_categorical_features(punctuation_counts_1, punctuation_counts_2)\n",
    "\n",
    "    #pos_tag (jac_sim):\n",
    "    pos_tag_features_1 = extract_pos_tag_features(para_1)\n",
    "    pos_tag_features_2 = extract_pos_tag_features(para_2)\n",
    "    jaccard_similarity_punct = compare_categorical_features(pos_tag_features_1, pos_tag_features_2)\n",
    "\n",
    "\n",
    "    return jaccard_similarity_word_freqs, tf_idf_similarity_score, sent_len_diff, jaccard_similarity_punct, jaccard_similarity_punct\n",
    "\n",
    "def extract_ease_sim(para1, para2):\n",
    "    ease_1 = extract_reading_ease(para1)\n",
    "    ease_2 = extract_reading_ease(para2)\n",
    "\n",
    "    return ease_2 - ease_1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\milos\\miniconda3\\envs\\prpro-2024\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def calculate_base_features(df):\n",
    "    #df_100 = df.head(100)\n",
    "    df['f1'] = pd.Series()\n",
    "    df['f2'] = pd.Series()\n",
    "    df['f3'] = pd.Series()\n",
    "    df['f4'] = pd.Series()\n",
    "    df['f5'] = pd.Series()\n",
    "    df['f6'] = pd.Series()\n",
    "\n",
    "    f1 = []\n",
    "    f2 = []\n",
    "    f3 = []\n",
    "    f4 = []\n",
    "    f5 = []\n",
    "    f6 = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        features_in_row = list(extract_similarity(df[\"paragraph1\"][i], df[\"paragraph2\"][i]))\n",
    "        f1.append(features_in_row[0])\n",
    "        f2.append(features_in_row[1])\n",
    "        f3.append(features_in_row[2])\n",
    "        f4.append(features_in_row[3])\n",
    "        f5.append(features_in_row[4]) \n",
    "        f6_val  = extract_ease_sim(df[\"paragraph1\"][i], df[\"paragraph2\"][i])\n",
    "        f6.append(f6_val)\n",
    "\n",
    "    df['f1'] = f1\n",
    "    df['f2'] = f2\n",
    "    df['f3'] = f3\n",
    "    df['f4'] = f4\n",
    "    df['f5'] = f5\n",
    "    df['f6'] = f6\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tfidf on whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tfidf_col(df):\n",
    "    tfidf_matrix_1 = create_tfidf_matrix(df[\"paragraph1\"])\n",
    "    tfidf_matrix_2 = create_tfidf_matrix(df[\"paragraph2\"])\n",
    "\n",
    "    common_columns = tfidf_matrix_1.columns.intersection(tfidf_matrix_2.columns)\n",
    "\n",
    "\n",
    "    #tfidf_matrix_1 = tfidf_matrix_1.head(10000)\n",
    "    #tfidf_matrix_2 = tfidf_matrix_2.head(10000)\n",
    "\n",
    "    tfidf_matrix_1_filtered = tfidf_matrix_1[common_columns].astype(np.float16)\n",
    "    tfidf_matrix_2_filtered = tfidf_matrix_2[common_columns].astype(np.float16)\n",
    "\n",
    "\n",
    "    cosine_similarities = tfidf_similarities(tfidf_matrix_1_filtered, tfidf_matrix_2_filtered)\n",
    "\n",
    "    df['f7'] = cosine_similarities\n",
    "\n",
    "    df['f7'] = df['f7'].fillna(0) \n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_all_features(df):\n",
    "    calculate_base_features(df)\n",
    "    calculate_tfidf_col(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_dataframe = calculate_all_features(df_train)\n",
    "train_features_dataframe.to_csv(\"train_features_dataframe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features_dataframe = calculate_all_features(df_test_medium)\n",
    "test_features_dataframe.to_csv(\"test_features_dataframe_medium.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "1    31496\n",
      "0    20466\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "1    31496\n",
      "0    31496\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "X = df[[\"f1\", \"f2\", \"f3\", \"f4\", \"f5\", \"f6\", \"f7\"]]\n",
    "y = df[\"label\"]\n",
    "\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Check the new class distribution\n",
    "print(pd.Series(y_resampled).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform it\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "#df.head(10)[[\"f1\", \"f2\", \"f3\", \"f4\", \"f5\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First model - linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score: 0.6392087392973133\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "f1_score_reg = f1_score(y_test, predictions)\n",
    "print(\"f1_score:\", f1_score_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second model - SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "for kernel in ['rbf']:\n",
    "    svm_model = SVC(kernel= kernel)  \n",
    "    svm_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    y_pred = svm_model.predict(X_test_scaled)\n",
    "\n",
    "    f1_score_svm = f1_score(y_test, y_pred)\n",
    "    print(f\"f1_score svm, kernel: {kernel}:\", f1_score_svm)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prpro-2024",
   "language": "python",
   "name": "prpro-2024"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
