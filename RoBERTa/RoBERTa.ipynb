{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preperation Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(default_directory: str):\n",
    "    \"\"\"\n",
    "    Preprocess the data and labels to provide text pair\n",
    "\n",
    "    Args:\n",
    "        default_directory: Default directory for both training and validation data\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary contained processed training and validation sets\n",
    "    \"\"\"\n",
    "\n",
    "    # Defining dictionary\n",
    "    data_dict = {\n",
    "        \"train\": [],\n",
    "        \"validation\": []\n",
    "    }\n",
    "\n",
    "    # Iterate through folders\n",
    "    for split in [\"train\", \"validation\"]:\n",
    "        for difficulty in [\"easy\", \"medium\", \"hard\"]:\n",
    "            # Difficulty dict\n",
    "            difficulty_dict = os.path.join(default_directory, difficulty)\n",
    "            # Set current directory [train, validation]\n",
    "            current_directory = os.path.join(difficulty_dict, split)\n",
    "            \n",
    "            # Iterate over all filenames\n",
    "            for filename in os.listdir(current_directory):\n",
    "                # Only work on .txt files\n",
    "                if filename.endswith(\".txt\"):\n",
    "                    text_path = os.path.join(current_directory, filename)\n",
    "                    label_path = os.path.join(current_directory, \"truth-\" + filename.replace(\".txt\", \".json\"))\n",
    "\n",
    "                    # Open an process the files\n",
    "                    # Text files\n",
    "                    with open(text_path) as f:\n",
    "                        text = f.read()\n",
    "                    paragraphs = text.strip().split(\"\\n\")\n",
    "                    # Labels\n",
    "                    with open(label_path) as f:\n",
    "                        object = json.load(f)\n",
    "                    labels = object.get(\"changes\")\n",
    "                    \n",
    "                    # print(paragraphs)\n",
    "                    # print(labels)\n",
    "\n",
    "                    # Error handling by removing badly formatted files\n",
    "                    if len(labels) != len(paragraphs)-1:\n",
    "                        os.remove(text_path)\n",
    "                        os.remove(label_path)\n",
    "                        print(\"Removed bad formatted files\")\n",
    "                    \n",
    "                    # Fill up data_dict\n",
    "                    for i in range(1, len(paragraphs)):\n",
    "                        data_dict[split].append([paragraphs[i-1], paragraphs[i], labels[i-1]])\n",
    "    return data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_directory = \"../pan24-multi-author-analysis\"\n",
    "data_dict = create_datasets(default_directory=default_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph1</th>\n",
       "      <th>paragraph2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just like who knew the Taliban would be just a...</td>\n",
       "      <td>Also the EU wasn’t born out of a “let’s preven...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Also the EU wasn’t born out of a “let’s preven...</td>\n",
       "      <td>Their money? What money did they have before t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Their money? What money did they have before t...</td>\n",
       "      <td>Well...yeah actually. It kinda sucks to listen...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In general, be courteous to others. Debate/dis...</td>\n",
       "      <td>Sigh. There are so many right wing monsters we...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sigh. There are so many right wing monsters we...</td>\n",
       "      <td>r/politics is currently accepting new moderato...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          paragraph1  \\\n",
       "0  Just like who knew the Taliban would be just a...   \n",
       "1  Also the EU wasn’t born out of a “let’s preven...   \n",
       "2  Their money? What money did they have before t...   \n",
       "3  In general, be courteous to others. Debate/dis...   \n",
       "4  Sigh. There are so many right wing monsters we...   \n",
       "\n",
       "                                          paragraph2  label  \n",
       "0  Also the EU wasn’t born out of a “let’s preven...      1  \n",
       "1  Their money? What money did they have before t...      1  \n",
       "2  Well...yeah actually. It kinda sucks to listen...      1  \n",
       "3  Sigh. There are so many right wing monsters we...      1  \n",
       "4  r/politics is currently accepting new moderato...      1  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data_dict.get(\"train\"), columns=[\"paragraph1\", \"paragraph2\", \"label\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dependencies\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# # Creating custom dataset\n",
    "# class CustomParagraphsDataset(Dataset):\n",
    "#     \"\"\" \n",
    "#     Dataset containing pairs of data points \n",
    "\n",
    "#     Args:\n",
    "#         Dataset\n",
    "\n",
    "#     Return:\n",
    "#         Sample dictionary\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Initialize the variables\n",
    "#     def __init__(self, train=True, transform=None):\n",
    "#         columns = [\"paragraph1\", \"paragraph2\", \"label\"]\n",
    "#         self.dataframe = pd.DataFrame(data_dict.get(\"train\"), columns=columns) if train else pd.DataFrame(data_dict.get(\"validation\"), columns=columns)\n",
    "#         self.transform = transform\n",
    "\n",
    "#     # Function for returning size of dataset\n",
    "#     def __len__(self):\n",
    "#         return len(self.dataframe)\n",
    "\n",
    "#     # Function for retrieving item\n",
    "#     def __getitem__(self, idx):\n",
    "#         # Convert index to list\n",
    "#         if torch.is_tensor(idx):\n",
    "#             idx = idx.tolist()\n",
    "        \n",
    "#         # Defining sample\n",
    "#         sample = {\n",
    "#             \"paragraph1\": self.dataframe.iloc[idx][\"paragraph1\"],\n",
    "#             \"paragraph2\": self.dataframe.iloc[idx][\"paragraph2\"],\n",
    "#             \"label\": self.dataframe.iloc[idx][\"label\"]\n",
    "#         }\n",
    "\n",
    "#         # Transform check\n",
    "#         if self.transform:\n",
    "#             sample = self.transform(sample)\n",
    "        \n",
    "#         return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST START ------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets.dataset_dict import DatasetDict\n",
    "# from datasets import Dataset\n",
    "\n",
    "# columns = [\"paragraph1\", \"paragraph2\", \"label\"]\n",
    "\n",
    "# d = {\n",
    "#     \"train\": Dataset.from_dict({\n",
    "#         \"paragraph1\": pd.DataFrame(data_dict.get(\"train\"), columns=columns)[\"paragraph1\"],\n",
    "#         \"paragraph2\": pd.DataFrame(data_dict.get(\"train\"), columns=columns)[\"paragraph2\"],\n",
    "#         \"label\": pd.DataFrame(data_dict.get(\"train\"), columns=columns)[\"label\"]\n",
    "#     })\n",
    "# }\n",
    "\n",
    "# raw = DatasetDict(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = CustomParagraphsDataset()\n",
    "# validation_dataset = CustomParagraphsDataset(train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing dataset creation dependenceis\n",
    "from datasets import DatasetDict, Dataset\n",
    "\n",
    "# Defining column names\n",
    "columns = [\"paragraph1\", \"paragraph2\", \"label\"]\n",
    "\n",
    "# Creating raw dataset\n",
    "raw_datasets = DatasetDict({\n",
    "    \"train\": Dataset.from_dict({\n",
    "        \"paragraph1\": pd.DataFrame(data_dict.get(\"train\"), columns=columns)[\"paragraph1\"],\n",
    "        \"paragraph2\": pd.DataFrame(data_dict.get(\"train\"), columns=columns)[\"paragraph2\"],\n",
    "        \"label\": pd.DataFrame(data_dict.get(\"train\"), columns=columns)[\"label\"]\n",
    "    }),\n",
    "    \"validation\": Dataset.from_dict({\n",
    "        \"paragraph1\": pd.DataFrame(data_dict.get(\"validation\"), columns=columns)[\"paragraph1\"],\n",
    "        \"paragraph2\": pd.DataFrame(data_dict.get(\"validation\"), columns=columns)[\"paragraph2\"],\n",
    "        \"label\": pd.DataFrame(data_dict.get(\"validation\"), columns=columns)[\"label\"]\n",
    "    })\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['paragraph1', 'paragraph2', 'label'],\n",
       "        num_rows: 51962\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['paragraph1', 'paragraph2', 'label'],\n",
       "        num_rows: 11194\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing and Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "reference = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(sample):\n",
    "    return tokenizer(\n",
    "        sample[\"paragraph1\"],\n",
    "        sample[\"paragraph2\"],\n",
    "        truncation=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 51962/51962 [00:04<00:00, 12081.06 examples/s]\n",
      "Map: 100%|██████████| 11194/11194 [00:01<00:00, 10937.39 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': ['paragraph1', 'paragraph2', 'label', 'input_ids', 'attention_mask'], 'validation': ['paragraph1', 'paragraph2', 'label', 'input_ids', 'attention_mask']}\n"
     ]
    }
   ],
   "source": [
    "# Testing mapping\n",
    "print(tokenized_datasets.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nONLY FOR WHEN NOT USING THE TRAINER API\\n'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "ONLY FOR WHEN NOT USING THE TRAINER API\n",
    "\"\"\"\n",
    "# Post process removal\n",
    "# for key in tokenized_datasets.keys():\n",
    "#     tokenized_datasets[key] = tokenized_datasets[key].remove_columns([\"paragraph1\", \"paragraph2\"])\n",
    "#     tokenized_datasets[key] = tokenized_datasets[key].rename_column(\"label\", \"labels\")\n",
    "#     tokenized_datasets[key] = tokenized_datasets[key].with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['paragraph1', 'paragraph2', 'label', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 51962\n",
       "})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model(MODEL_NAME: str, num_labels: int):\n",
    "    from transformers import AutoModelForSequenceClassification\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        pretrained_model_name_or_path=MODEL_NAME,\n",
    "        num_labels=num_labels\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"roberta-base\"\n",
    "num_labels = len(pd.unique(df[\"label\"]))\n",
    "model = setup_model(MODEL_NAME=MODEL_NAME, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments for training\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"trainer\", \n",
    "    per_device_eval_batch_size=32,\n",
    "    per_device_train_batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from transformers import RobertaForSequenceClassification, RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_model(MODEL_NAME: str):\n",
    "#     \"\"\"\n",
    "#     Creates a RoBERTa model for the writing-style classification\n",
    "\n",
    "#     Args:\n",
    "#         model_name: Name of pre-trained model\n",
    "#     Returns:\n",
    "#         Model and tokenizer\n",
    "#     \"\"\"\n",
    "#     model = RobertaForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "#     tokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME)\n",
    "#     return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_NAME = \"roberta-base\"\n",
    "# model, tokenizer = create_model(MODEL_NAME=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_encodings = tokenizer(\n",
    "#     data_dict.get(\"train\")[0][0],\n",
    "#     truncuation=True,\n",
    "#     padding=True\n",
    "# )\n",
    "# train_encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import TrainingArguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"roberta_trainer\",\n",
    "#     per_device_train_batch_size=32,\n",
    "#     per_device_eval_batch_size=32,\n",
    "#     num_train_epochs=5,\n",
    "#     learning_rate=1e-3\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "writing-style-detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
