{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaModel, RobertaTokenizer, RobertaConfig\n",
    "import json\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(default_directory: str):\n",
    "    \"\"\"\n",
    "    Preprocess the data and labels to provide text pair\n",
    "\n",
    "    Args:\n",
    "        default_directory: Default directory for both training and validation data\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary contained processed training and validation sets\n",
    "    \"\"\"\n",
    "\n",
    "    # Defining dictionary\n",
    "    data_dict = {\n",
    "        \"train\": [],\n",
    "        \"validation\": []\n",
    "    }\n",
    "\n",
    "    # Iterate through folders\n",
    "    for split in [\"train\", \"validation\"]:\n",
    "        for difficulty in [\"easy\", \"medium\", \"hard\"]:\n",
    "            # Difficulty dict\n",
    "            difficulty_dict = os.path.join(default_directory, difficulty)\n",
    "            # Set current directory [train, validation]\n",
    "            current_directory = os.path.join(difficulty_dict, split)\n",
    "            \n",
    "            # Iterate over all filenames\n",
    "            for filename in os.listdir(current_directory):\n",
    "                # Only work on .txt files\n",
    "                if filename.endswith(\".txt\"):\n",
    "                    text_path = os.path.join(current_directory, filename)\n",
    "                    label_path = os.path.join(current_directory, \"truth-\" + filename.replace(\".txt\", \".json\"))\n",
    "\n",
    "                    # Open an process the files\n",
    "                    # Text files\n",
    "                    with open(text_path) as f:\n",
    "                        text = f.read()\n",
    "                    paragraphs = text.strip().split(\"\\n\")\n",
    "                    # Labels\n",
    "                    with open(label_path) as f:\n",
    "                        object = json.load(f)\n",
    "                    labels = object.get(\"changes\")\n",
    "                    \n",
    "                    # print(paragraphs)\n",
    "                    # print(labels)\n",
    "\n",
    "                    # Error handling by removing badly formatted files\n",
    "                    if len(labels) != len(paragraphs)-1:\n",
    "                        os.remove(text_path)\n",
    "                        os.remove(label_path)\n",
    "                        print(\"Removed bad formatted files\")\n",
    "\n",
    "                    # Split each paragraph into tokens\n",
    "                    processed_paragraphs = [paragraph.split() for paragraph in paragraphs]\n",
    "                    \n",
    "                    # Fill up data_dict\n",
    "                    for i in range(1, len(paragraphs)):\n",
    "                        #print((paragraphs[i-1], paragraphs[i], labels[i-1]))\n",
    "                        data_dict[split].append([paragraphs[i-1], paragraphs[i], labels[i-1]])\n",
    "    return data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_directory = \"../pan24-multi-author-analysis\"\n",
    "data_dict = create_datasets(default_directory=default_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51962\n"
     ]
    }
   ],
   "source": [
    "print(len(data_dict.get(\"train\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "writing-style-detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
